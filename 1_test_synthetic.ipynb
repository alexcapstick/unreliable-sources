{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './outputs/graphs/'\n",
    "results_path = './outputs/synthetic_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import graph_code\n",
    "import os\n",
    "import functools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_n_dict = {\n",
    "    'lap_n_25': 'LAP Model (Ours)',\n",
    "    'lap_n_20': 'LAP Model (Ours), LAPN 20',\n",
    "    'lap_n_50': 'LAP Model (Ours), LAPN 50',\n",
    "}\n",
    "\n",
    "run_model_map = functools.partial(graph_code.run_model_map, lap_n_dict=lap_n_dict)\n",
    "\n",
    "experiment_dict = {\n",
    "    'Original\\nData' : [\n",
    "        'Conv3Net-no_c-drstd-results.csv'\n",
    "    ],\n",
    "    'Chunk\\nShuffle' : [\n",
    "        'Conv3Net-c_cs-drstd-results.csv', 'Conv3Net-c_cs_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Random\\nLabel' : [\n",
    "        'Conv3Net-c_rl-drstd-results.csv', 'Conv3Net-c_rl_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Batch\\nLabel\\nShuffle' : [\n",
    "        'Conv3Net-c_lbs-drstd-results.csv', 'Conv3Net-c_lbs_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Batch\\nLabel\\nFlip' : [\n",
    "        'Conv3Net-c_lbf-drstd-results.csv', 'Conv3Net-c_lbf_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Added\\nNoise' : [\n",
    "        'Conv3Net-c_ns-drstd-results.csv', 'Conv3Net-c_ns_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Replace\\nWith\\nNoise' : [\n",
    "        'Conv3Net-c_no-drstd-results.csv', 'Conv3Net-c_no_srb-drstd-results.csv'\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for experiment, files in experiment_dict.items():\n",
    "    for file in files:\n",
    "        results_temp = pd.read_csv(os.path.join(results_path, file))\n",
    "        results_temp['Corruption Type'] = experiment\n",
    "        results = pd.concat([results, results_temp])\n",
    "    results = graph_code.expand_run_names(results)\n",
    "    results['Model Name'] = results['Run'].map(run_model_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## federated learning results\n",
    "fed_arfl_folder = os.path.join(results_path, 'baseline', 'arfl', 'cifar10')\n",
    "tbl = graph_code.TensorboardLoad(fed_arfl_folder, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files:   1%|▏▏▏▏▏▏▏▏▏▏| 1/71 [00:00<00:00, 316.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|▉▉▉▉▉▉▉▉▉▉| 71/71 [00:00<00:00, 81.52it/s] \n"
     ]
    }
   ],
   "source": [
    "fed_arfl_results = tbl.scalars(tags='weighted_average_test_results_accuracy')[1]\n",
    "\n",
    "fed_arfl_results = (fed_arfl_results\n",
    ".drop(['run', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4'], axis=1)\n",
    ".rename(\n",
    "    {'level_6': 'Run', 'value': 'Value', 'step': 'Step', 'tag': 'Metric'},\n",
    "    axis=1)\n",
    ")\n",
    "\n",
    "fed_arfl_results = graph_code.expand_run_names(fed_arfl_results)\n",
    "fed_arfl_results = fed_arfl_results.query(\"Step == 9024\")[['Run', 'Value', 'Seed']]\n",
    "fed_arfl_results['Corruption Type'] = fed_arfl_results['Run'].map(graph_code.run_corrupt_map)\n",
    "fed_arfl_results['Model Name'] = 'Fed ARFL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_cifar10 = results[\n",
    "    (results['Metric'].isin(['Accuracy']))\n",
    "    & (results['Number of Epochs'] == '25')\n",
    "    & (results['Number of Corrupt Sources'].isin(['0', '4']))\n",
    "    & (results['LAP N'].isin(['20', '25', '50']))\n",
    "    & (results['Depression Strength'].isin(['0.0', '1.0']))\n",
    "    & (results['Strictness'].isin(['0.8']))\n",
    "].copy()\n",
    "\n",
    "data_plot_cifar10 = pd.concat([data_plot_cifar10, fed_arfl_results])\n",
    "data_plot_cifar10['Accuracy (%)'] = 100*data_plot_cifar10['Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_n_dict = {\n",
    "    'lap_n_25': 'LAP Model (Ours)',\n",
    "    'lap_n_20': 'LAP Model (Ours), LAPN 20',\n",
    "    'lap_n_50': 'LAP Model (Ours), LAPN 50',\n",
    "}\n",
    "\n",
    "run_model_map = functools.partial(graph_code.run_model_map, lap_n_dict=lap_n_dict)\n",
    "\n",
    "experiment_dict = {\n",
    "    'Original\\nData' : [\n",
    "        'Conv3Net_100-no_c-drstd-results.csv'\n",
    "    ],\n",
    "    'Chunk\\nShuffle' : [\n",
    "        'Conv3Net_100-c_cs-drstd-results.csv', 'Conv3Net_100-c_cs_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Random\\nLabel' : [\n",
    "        'Conv3Net_100-c_rl-drstd-results.csv', 'Conv3Net_100-c_rl_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Batch\\nLabel\\nShuffle' : [\n",
    "        'Conv3Net_100-c_lbs-drstd-results.csv', 'Conv3Net_100-c_lbs_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Batch\\nLabel\\nFlip' : [\n",
    "        'Conv3Net_100-c_lbf-drstd-results.csv', 'Conv3Net_100-c_lbf_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Added\\nNoise' : [\n",
    "        'Conv3Net_100-c_ns-drstd-results.csv', 'Conv3Net_100-c_ns_srb-drstd-results.csv'\n",
    "    ],\n",
    "    'Replace\\nWith\\nNoise' : [\n",
    "        'Conv3Net_100-c_no-drstd-results.csv', 'Conv3Net_100-c_no_srb-drstd-results.csv'\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for experiment, files in experiment_dict.items():\n",
    "    for file in files:\n",
    "        results_temp = pd.read_csv(os.path.join(results_path, file))\n",
    "        results_temp['Corruption Type'] = experiment\n",
    "        results = pd.concat([results, results_temp])\n",
    "    results = graph_code.expand_run_names(results)\n",
    "    results['Model Name'] = results['Run'].map(run_model_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## federated learning results\n",
    "fed_arfl_folder = os.path.join(results_path, 'baseline', 'arfl', 'cifar100')\n",
    "tbl = graph_code.TensorboardLoad(fed_arfl_folder, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|▉▉▉▉▉▉▉▉▉▉| 71/71 [00:00<00:00, 80.47it/s] \n"
     ]
    }
   ],
   "source": [
    "fed_arfl_results = tbl.scalars(tags='weighted_average_test_results_top_5_acc')[1]\n",
    "\n",
    "fed_arfl_results = (fed_arfl_results\n",
    ".drop(['run', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4'], axis=1)\n",
    ".rename(\n",
    "    {'level_6': 'Run', 'value': 'Value', 'step': 'Step', 'tag': 'Metric'},\n",
    "    axis=1)\n",
    ")\n",
    "fed_arfl_results = graph_code.expand_run_names(fed_arfl_results)\n",
    "fed_arfl_results = fed_arfl_results.query(\"Step == 14664\")[['Run', 'Value', 'Seed']]\n",
    "fed_arfl_results['Corruption Type'] = fed_arfl_results['Run'].map(graph_code.run_corrupt_map)\n",
    "fed_arfl_results['Model Name'] = 'Fed ARFL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_cifar100 = results[\n",
    "    (results['Metric'].isin(['Top 5 Accuracy']))\n",
    "    & (results['Number of Epochs'] == '40')\n",
    "    & (results['Number of Corrupt Sources'].isin(['2']))\n",
    "    & (results['LAP N'].isin(['25']))\n",
    "    & (results['Depression Strength'].isin(['0.0', '1.0']))\n",
    "    & (results['Strictness'].isin(['0.8']))\n",
    "].copy()\n",
    "\n",
    "data_plot_cifar100 = pd.concat([data_plot_cifar100, fed_arfl_results])\n",
    "data_plot_cifar100['Top 5 Accuracy (%)'] = 100*data_plot_cifar100['Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_n_dict = {\n",
    "    'lap_n_25': 'LAP Model (Ours), LAPN 25',\n",
    "    'lap_n_20': 'LAP Model (Ours), LAPN 20',\n",
    "    'lap_n_50': 'LAP Model (Ours)',\n",
    "}\n",
    "\n",
    "run_model_map = functools.partial(graph_code.run_model_map, lap_n_dict=lap_n_dict)\n",
    "\n",
    "experiment_dict = {\n",
    "    'Original\\nData' :  ['MLP-no_c-drstd-results.csv'],\n",
    "    'Chunk\\nShuffle' : ['MLP-c_cs-drstd-results.csv', 'MLP-c_cs_srb-drstd-results.csv'],\n",
    "    'Random\\nLabel' : ['MLP-c_rl-drstd-results.csv', 'MLP-c_rl_srb-drstd-results.csv'],\n",
    "    'Batch\\nLabel\\nShuffle' : ['MLP-c_lbs-drstd-results.csv', 'MLP-c_lbs_srb-drstd-results.csv'],\n",
    "    'Batch\\nLabel\\nFlip' : ['MLP-c_lbf-drstd-results.csv', 'MLP-c_lbf_srb-drstd-results.csv'],\n",
    "    'Added\\nNoise' : ['MLP-c_ns-drstd-results.csv', 'MLP-c_ns_srb-drstd-results.csv'],\n",
    "    'Replace\\nWith\\nNoise' : ['MLP-c_no-drstd-results.csv', 'MLP-c_no_srb-drstd-results.csv'],\n",
    "    }\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for experiment, files in experiment_dict.items():\n",
    "    for file in files:\n",
    "        results_temp = pd.read_csv(os.path.join(results_path, file))\n",
    "        results_temp['Corruption Type'] = experiment\n",
    "        results = pd.concat([results, results_temp])\n",
    "    results = graph_code.expand_run_names(results)\n",
    "    results['Model Name'] = results['Run'].map(run_model_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## federated learning results\n",
    "fed_arfl_folder = os.path.join(results_path, 'baseline', 'arfl', 'fmnist')\n",
    "tbl = graph_code.TensorboardLoad(fed_arfl_folder, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Files: 100%|▉▉▉▉▉▉▉▉▉▉| 71/71 [00:00<00:00, 166.14it/s]\n"
     ]
    }
   ],
   "source": [
    "fed_arfl_results = tbl.scalars(tags='weighted_average_test_results_accuracy')[1]\n",
    "\n",
    "fed_arfl_results = (fed_arfl_results\n",
    "    .drop(['run', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4'], axis=1)\n",
    "    .rename(\n",
    "        {'level_6': 'Run', 'value': 'Value', 'step': 'Step', 'tag': 'Metric'},\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "fed_arfl_results = graph_code.expand_run_names(fed_arfl_results)\n",
    "fed_arfl_results = fed_arfl_results.query(\"Step == 14664\")[['Run', 'Value', 'Seed']]\n",
    "fed_arfl_results['Corruption Type'] = fed_arfl_results['Run'].map(graph_code.run_corrupt_map)\n",
    "fed_arfl_results['Model Name'] = 'Fed ARFL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_fmnist = results[\n",
    "    (results['Metric'].isin(['Accuracy']))\n",
    "    & (results['Number of Epochs'] == '40')\n",
    "    & (results['Number of Corrupt Sources'].isin(['0', '6']))\n",
    "    & (results['LAP N'].isin(['20', '25', '50']))\n",
    "    & (results['Depression Strength'].isin(['0.0', '1.0']))\n",
    "    & (results['Strictness'].isin(['0.8']))\n",
    "].copy()\n",
    "\n",
    "data_plot_fmnist = pd.concat([data_plot_fmnist, fed_arfl_results])\n",
    "data_plot_fmnist['Accuracy (%)'] = 100*data_plot_fmnist['Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot_cifar10['Dataset'] = 'CIFAR-10'\n",
    "data_plot_cifar100['Dataset'] = 'CIFAR-100'\n",
    "data_plot_fmnist['Dataset'] = 'F-MNIST'\n",
    "\n",
    "data_plot_cifar100['Accuracy (%)'] = data_plot_cifar100['Top 5 Accuracy (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = pd.concat([data_plot_cifar10, data_plot_cifar100, data_plot_fmnist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data_plot[['Dataset', 'Corruption Type', 'Model Name', 'Accuracy (%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_results(path):\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    experiment_dict = {\n",
    "        'no_c': 'Original\\nData',\n",
    "        'c_cs': 'Chunk\\nShuffle',\n",
    "        'c_rl': 'Random\\nLabel',\n",
    "        'c_lbs': 'Batch\\nLabel\\nShuffle',\n",
    "        'c_lbf': 'Batch\\nLabel\\nFlip',\n",
    "        'c_ns': 'Added\\nNoise',\n",
    "        'c_no': 'Replace\\nWith\\nNoise',\n",
    "    }\n",
    "\n",
    "    def get_correct_accuracy(df):\n",
    "        accuracy = df['Accuracy (%)'].map(lambda x: x[0]).copy()\n",
    "        top5_accuracy = df['Accuracy (%)'].map(lambda x: x[1]).copy()\n",
    "        index_top_5 = df['Dataset'].isin(['cifar100'])\n",
    "        accuracy[index_top_5] = top5_accuracy[index_top_5]\n",
    "        return accuracy\n",
    "\n",
    "    results = (\n",
    "        pd.concat(\n",
    "            {\n",
    "                k: pd.DataFrame(v).T for k, v in results.items()\n",
    "            }, \n",
    "            axis=0\n",
    "        )\n",
    "        .melt(var_name='Run', value_name='Accuracy (%)', ignore_index=False)\n",
    "        .rename_axis(['Dataset', 'Corruption Type'])   \n",
    "        .reset_index()\n",
    "        .assign(\n",
    "            **{\n",
    "                'Corruption Type': lambda x: x['Corruption Type'].map(experiment_dict)\n",
    "            }\n",
    "        )\n",
    "        .assign(\n",
    "            **{\n",
    "                'Accuracy (%)': lambda x: get_correct_accuracy(x)*100\n",
    "            }\n",
    "        )\n",
    "        .drop(columns=['Run'])\n",
    "        .replace(\n",
    "            {\n",
    "                'Dataset': {\n",
    "                'cifar10': 'CIFAR-10',\n",
    "                'cifar100': 'CIFAR-100',\n",
    "                'fmnist': 'F-MNIST'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idpa_path = os.path.join(\n",
    "    results_path,\n",
    "    'baseline',\n",
    "    'idpa',\n",
    "    'results.json'\n",
    ")\n",
    "\n",
    "idpa_results = (\n",
    "    load_baseline_results(idpa_path)\n",
    "    .assign(**{'Model Name': 'IDPA'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coteaching_path = os.path.join(\n",
    "    results_path,\n",
    "    'baseline',\n",
    "    'co-teaching',\n",
    "    'results.json'\n",
    ")\n",
    "\n",
    "coteaching_results = (\n",
    "    load_baseline_results(coteaching_path)\n",
    "    .assign(**{'Model Name': 'Co-teaching'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = pd.concat([data_plot, idpa_results, coteaching_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Corruption Type</th>\n",
       "      <th>Standard Model</th>\n",
       "      <th>Fed ARFL</th>\n",
       "      <th>IDPA</th>\n",
       "      <th>Co-teaching</th>\n",
       "      <th>LAP Model (Ours)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Original Data</td>\n",
       "      <td>\\textbf{67.89 ± 1.09}</td>\n",
       "      <td>61.32 ± 1.13</td>\n",
       "      <td>64.17 ± 1.92</td>\n",
       "      <td>67.19 ± 0.86</td>\n",
       "      <td>67.82 ± 1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Chunk Shuffle</td>\n",
       "      <td>63.53 ± 1.55</td>\n",
       "      <td>58.01 ± 1.54</td>\n",
       "      <td>58.11 ± 1.30</td>\n",
       "      <td>62.39 ± 0.65</td>\n",
       "      <td>\\textbf{64.22 ± 2.06}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Random Label</td>\n",
       "      <td>57.77 ± 1.52</td>\n",
       "      <td>57.71 ± 2.32</td>\n",
       "      <td>51.02 ± 2.10</td>\n",
       "      <td>54.35 ± 1.48</td>\n",
       "      <td>\\textbf{62.74 ± 1.84}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Batch Label Shuffle</td>\n",
       "      <td>57.65 ± 1.74</td>\n",
       "      <td>59.38 ± 1.25</td>\n",
       "      <td>51.71 ± 1.59</td>\n",
       "      <td>58.66 ± 0.67</td>\n",
       "      <td>\\textbf{63.06 ± 1.90}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Batch Label Flip</td>\n",
       "      <td>51.51 ± 2.43</td>\n",
       "      <td>59.21 ± 1.35</td>\n",
       "      <td>49.58 ± 1.67</td>\n",
       "      <td>51.82 ± 1.41</td>\n",
       "      <td>\\textbf{63.21 ± 2.35}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Added Noise</td>\n",
       "      <td>57.30 ± 1.60</td>\n",
       "      <td>51.57 ± 2.21</td>\n",
       "      <td>53.43 ± 2.14</td>\n",
       "      <td>57.64 ± 0.71</td>\n",
       "      <td>\\textbf{59.62 ± 1.91}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Replace With Noise</td>\n",
       "      <td>61.27 ± 2.66</td>\n",
       "      <td>55.19 ± 1.72</td>\n",
       "      <td>59.22 ± 1.66</td>\n",
       "      <td>\\textbf{62.27 ± 1.02}</td>\n",
       "      <td>61.52 ± 1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Original Data</td>\n",
       "      <td>\\textbf{64.46 ± 0.93}</td>\n",
       "      <td>58.25 ± 1.19</td>\n",
       "      <td>58.64 ± 0.96</td>\n",
       "      <td>61.42 ± 0.99</td>\n",
       "      <td>64.46 ± 0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Chunk Shuffle</td>\n",
       "      <td>61.87 ± 1.16</td>\n",
       "      <td>57.12 ± 0.90</td>\n",
       "      <td>53.49 ± 1.22</td>\n",
       "      <td>54.74 ± 2.15</td>\n",
       "      <td>\\textbf{62.56 ± 0.61}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Random Label</td>\n",
       "      <td>55.90 ± 1.11</td>\n",
       "      <td>56.49 ± 1.83</td>\n",
       "      <td>44.03 ± 1.31</td>\n",
       "      <td>45.08 ± 1.61</td>\n",
       "      <td>\\textbf{61.26 ± 1.02}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Batch Label Shuffle</td>\n",
       "      <td>57.67 ± 0.86</td>\n",
       "      <td>56.85 ± 1.13</td>\n",
       "      <td>46.37 ± 1.17</td>\n",
       "      <td>50.32 ± 1.00</td>\n",
       "      <td>\\textbf{61.62 ± 1.34}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Batch Label Flip</td>\n",
       "      <td>48.94 ± 2.13</td>\n",
       "      <td>56.34 ± 1.20</td>\n",
       "      <td>26.55 ± 7.71</td>\n",
       "      <td>24.97 ± 3.29</td>\n",
       "      <td>\\textbf{59.98 ± 1.06}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Added Noise</td>\n",
       "      <td>56.83 ± 0.91</td>\n",
       "      <td>53.49 ± 1.01</td>\n",
       "      <td>48.35 ± 1.48</td>\n",
       "      <td>50.58 ± 0.99</td>\n",
       "      <td>\\textbf{59.56 ± 0.99}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>Replace With Noise</td>\n",
       "      <td>\\textbf{60.06 ± 0.97}</td>\n",
       "      <td>55.00 ± 1.08</td>\n",
       "      <td>50.44 ± 2.24</td>\n",
       "      <td>53.00 ± 1.31</td>\n",
       "      <td>59.99 ± 1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Original Data</td>\n",
       "      <td>82.86 ± 1.51</td>\n",
       "      <td>80.95 ± 1.40</td>\n",
       "      <td>\\textbf{83.07 ± 1.06}</td>\n",
       "      <td>77.85 ± 1.94</td>\n",
       "      <td>82.86 ± 1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Chunk Shuffle</td>\n",
       "      <td>76.90 ± 2.00</td>\n",
       "      <td>76.10 ± 1.15</td>\n",
       "      <td>76.82 ± 1.88</td>\n",
       "      <td>73.79 ± 3.12</td>\n",
       "      <td>\\textbf{81.03 ± 2.27}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Random Label</td>\n",
       "      <td>73.47 ± 5.78</td>\n",
       "      <td>77.58 ± 1.69</td>\n",
       "      <td>74.30 ± 5.50</td>\n",
       "      <td>67.84 ± 6.00</td>\n",
       "      <td>\\textbf{77.91 ± 3.39}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Batch Label Shuffle</td>\n",
       "      <td>81.26 ± 1.72</td>\n",
       "      <td>78.35 ± 1.17</td>\n",
       "      <td>81.43 ± 0.54</td>\n",
       "      <td>78.79 ± 0.55</td>\n",
       "      <td>\\textbf{81.53 ± 1.70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Batch Label Flip</td>\n",
       "      <td>77.64 ± 3.43</td>\n",
       "      <td>77.84 ± 1.50</td>\n",
       "      <td>78.22 ± 3.04</td>\n",
       "      <td>75.96 ± 2.83</td>\n",
       "      <td>\\textbf{79.26 ± 3.78}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Added Noise</td>\n",
       "      <td>78.05 ± 1.96</td>\n",
       "      <td>73.12 ± 1.96</td>\n",
       "      <td>78.10 ± 1.69</td>\n",
       "      <td>75.26 ± 1.89</td>\n",
       "      <td>\\textbf{79.98 ± 3.83}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>F-MNIST</td>\n",
       "      <td>Replace With Noise</td>\n",
       "      <td>78.99 ± 2.20</td>\n",
       "      <td>74.29 ± 1.19</td>\n",
       "      <td>79.09 ± 1.72</td>\n",
       "      <td>75.82 ± 2.14</td>\n",
       "      <td>\\textbf{81.14 ± 2.25}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset      Corruption Type         Standard Model      Fed ARFL  \\\n",
       "0    CIFAR-10        Original Data  \\textbf{67.89 ± 1.09}  61.32 ± 1.13   \n",
       "1    CIFAR-10        Chunk Shuffle           63.53 ± 1.55  58.01 ± 1.54   \n",
       "2    CIFAR-10         Random Label           57.77 ± 1.52  57.71 ± 2.32   \n",
       "3    CIFAR-10  Batch Label Shuffle           57.65 ± 1.74  59.38 ± 1.25   \n",
       "4    CIFAR-10     Batch Label Flip           51.51 ± 2.43  59.21 ± 1.35   \n",
       "5    CIFAR-10          Added Noise           57.30 ± 1.60  51.57 ± 2.21   \n",
       "6    CIFAR-10   Replace With Noise           61.27 ± 2.66  55.19 ± 1.72   \n",
       "7   CIFAR-100        Original Data  \\textbf{64.46 ± 0.93}  58.25 ± 1.19   \n",
       "8   CIFAR-100        Chunk Shuffle           61.87 ± 1.16  57.12 ± 0.90   \n",
       "9   CIFAR-100         Random Label           55.90 ± 1.11  56.49 ± 1.83   \n",
       "10  CIFAR-100  Batch Label Shuffle           57.67 ± 0.86  56.85 ± 1.13   \n",
       "11  CIFAR-100     Batch Label Flip           48.94 ± 2.13  56.34 ± 1.20   \n",
       "12  CIFAR-100          Added Noise           56.83 ± 0.91  53.49 ± 1.01   \n",
       "13  CIFAR-100   Replace With Noise  \\textbf{60.06 ± 0.97}  55.00 ± 1.08   \n",
       "14    F-MNIST        Original Data           82.86 ± 1.51  80.95 ± 1.40   \n",
       "15    F-MNIST        Chunk Shuffle           76.90 ± 2.00  76.10 ± 1.15   \n",
       "16    F-MNIST         Random Label           73.47 ± 5.78  77.58 ± 1.69   \n",
       "17    F-MNIST  Batch Label Shuffle           81.26 ± 1.72  78.35 ± 1.17   \n",
       "18    F-MNIST     Batch Label Flip           77.64 ± 3.43  77.84 ± 1.50   \n",
       "19    F-MNIST          Added Noise           78.05 ± 1.96  73.12 ± 1.96   \n",
       "20    F-MNIST   Replace With Noise           78.99 ± 2.20  74.29 ± 1.19   \n",
       "\n",
       "                     IDPA            Co-teaching       LAP Model (Ours)  \n",
       "0            64.17 ± 1.92           67.19 ± 0.86           67.82 ± 1.31  \n",
       "1            58.11 ± 1.30           62.39 ± 0.65  \\textbf{64.22 ± 2.06}  \n",
       "2            51.02 ± 2.10           54.35 ± 1.48  \\textbf{62.74 ± 1.84}  \n",
       "3            51.71 ± 1.59           58.66 ± 0.67  \\textbf{63.06 ± 1.90}  \n",
       "4            49.58 ± 1.67           51.82 ± 1.41  \\textbf{63.21 ± 2.35}  \n",
       "5            53.43 ± 2.14           57.64 ± 0.71  \\textbf{59.62 ± 1.91}  \n",
       "6            59.22 ± 1.66  \\textbf{62.27 ± 1.02}           61.52 ± 1.87  \n",
       "7            58.64 ± 0.96           61.42 ± 0.99           64.46 ± 0.93  \n",
       "8            53.49 ± 1.22           54.74 ± 2.15  \\textbf{62.56 ± 0.61}  \n",
       "9            44.03 ± 1.31           45.08 ± 1.61  \\textbf{61.26 ± 1.02}  \n",
       "10           46.37 ± 1.17           50.32 ± 1.00  \\textbf{61.62 ± 1.34}  \n",
       "11           26.55 ± 7.71           24.97 ± 3.29  \\textbf{59.98 ± 1.06}  \n",
       "12           48.35 ± 1.48           50.58 ± 0.99  \\textbf{59.56 ± 0.99}  \n",
       "13           50.44 ± 2.24           53.00 ± 1.31           59.99 ± 1.26  \n",
       "14  \\textbf{83.07 ± 1.06}           77.85 ± 1.94           82.86 ± 1.51  \n",
       "15           76.82 ± 1.88           73.79 ± 3.12  \\textbf{81.03 ± 2.27}  \n",
       "16           74.30 ± 5.50           67.84 ± 6.00  \\textbf{77.91 ± 3.39}  \n",
       "17           81.43 ± 0.54           78.79 ± 0.55  \\textbf{81.53 ± 1.70}  \n",
       "18           78.22 ± 3.04           75.96 ± 2.83  \\textbf{79.26 ± 3.78}  \n",
       "19           78.10 ± 1.69           75.26 ± 1.89  \\textbf{79.98 ± 3.83}  \n",
       "20           79.09 ± 1.72           75.82 ± 2.14  \\textbf{81.14 ± 2.25}  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bold_max_value(x, model_names):\n",
    "    x = x.copy()\n",
    "    len_cols = x.shape[0]\n",
    "    n_models = len(model_names)\n",
    "    idx_bold = (\n",
    "        x\n",
    "        [-n_models:]\n",
    "        .str.replace(\" \", \"\")\n",
    "        .str.split(\"±\")\n",
    "        .str[0]\n",
    "        .argmax()\n",
    "    )\n",
    "    x.iloc[idx_bold+len_cols-n_models] = '\\\\textbf{' + x.iloc[idx_bold+len_cols-n_models] + '}'\n",
    "    \n",
    "    return x\n",
    "\n",
    "model_names = [\n",
    "    'Standard Model',\n",
    "    'Fed ARFL',\n",
    "    'IDPA',\n",
    "    'Co-teaching',\n",
    "    'LAP Model (Ours)',\n",
    "]\n",
    "\n",
    "corruption_type_order = [\n",
    "    'Original\\nData',\n",
    "    'Chunk\\nShuffle',\n",
    "    'Random\\nLabel',\n",
    "    'Batch\\nLabel\\nShuffle',\n",
    "    'Batch\\nLabel\\nFlip',\n",
    "    'Added\\nNoise',\n",
    "    'Replace\\nWith\\nNoise',\n",
    "]\n",
    "\n",
    "dataset_order = [\n",
    "    'CIFAR-10',\n",
    "    'CIFAR-100',\n",
    "    'F-MNIST',\n",
    "]\n",
    "\n",
    "full_synthetic_results = (\n",
    "    data_plot\n",
    "    .loc[lambda x: x[\"Model Name\"].isin(model_names)]\n",
    "    .groupby(['Dataset', 'Corruption Type', 'Model Name'])\n",
    "    .agg(['mean', 'std'])\n",
    "    .assign(result = (\n",
    "        lambda x: \n",
    "        x['Accuracy (%)']['mean'].map('{:.2f}'.format)\n",
    "        + ' ± ' \n",
    "        + x['Accuracy (%)']['std'].map('{:.2f}'.format)\n",
    "    ))\n",
    "    ['result']\n",
    "    .unstack(level='Model Name')\n",
    "    .reindex(\n",
    "        [\n",
    "            (ds, ct)  for ds in dataset_order for ct in corruption_type_order\n",
    "        ]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename_axis(index=None, columns=None)\n",
    "    .replace(\n",
    "        {\n",
    "            \"Corruption Type\": {\n",
    "                'Original\\nData': 'Original Data',\n",
    "                'Chunk\\nShuffle': 'Chunk Shuffle',\n",
    "                'Random\\nLabel': 'Random Label',\n",
    "                'Batch\\nLabel\\nShuffle': 'Batch Label Shuffle',\n",
    "                'Batch\\nLabel\\nFlip': 'Batch Label Flip',\n",
    "                'Added\\nNoise': 'Added Noise',\n",
    "                'Replace\\nWith\\nNoise': 'Replace With Noise',\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    [\n",
    "        ['Dataset', 'Corruption Type'] + model_names\n",
    "    ]\n",
    "    .apply(\n",
    "        bold_max_value,\n",
    "        model_names = model_names,\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "full_synthetic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "Dataset & Corruption Type & Standard Model & Fed ARFL & IDPA & Co-teaching & LAP Model (Ours) \\\\\n",
      "\\midrule\n",
      "CIFAR-10 & Original Data & \\textbf{67.89 ± 1.09} & 61.32 ± 1.13 & 64.17 ± 1.92 & 67.19 ± 0.86 & 67.82 ± 1.31 \\\\\n",
      "CIFAR-10 & Chunk Shuffle & 63.53 ± 1.55 & 58.01 ± 1.54 & 58.11 ± 1.30 & 62.39 ± 0.65 & \\textbf{64.22 ± 2.06} \\\\\n",
      "CIFAR-10 & Random Label & 57.77 ± 1.52 & 57.71 ± 2.32 & 51.02 ± 2.10 & 54.35 ± 1.48 & \\textbf{62.74 ± 1.84} \\\\\n",
      "CIFAR-10 & Batch Label Shuffle & 57.65 ± 1.74 & 59.38 ± 1.25 & 51.71 ± 1.59 & 58.66 ± 0.67 & \\textbf{63.06 ± 1.90} \\\\\n",
      "CIFAR-10 & Batch Label Flip & 51.51 ± 2.43 & 59.21 ± 1.35 & 49.58 ± 1.67 & 51.82 ± 1.41 & \\textbf{63.21 ± 2.35} \\\\\n",
      "CIFAR-10 & Added Noise & 57.30 ± 1.60 & 51.57 ± 2.21 & 53.43 ± 2.14 & 57.64 ± 0.71 & \\textbf{59.62 ± 1.91} \\\\\n",
      "CIFAR-10 & Replace With Noise & 61.27 ± 2.66 & 55.19 ± 1.72 & 59.22 ± 1.66 & \\textbf{62.27 ± 1.02} & 61.52 ± 1.87 \\\\\n",
      "CIFAR-100 & Original Data & \\textbf{64.46 ± 0.93} & 58.25 ± 1.19 & 58.64 ± 0.96 & 61.42 ± 0.99 & 64.46 ± 0.93 \\\\\n",
      "CIFAR-100 & Chunk Shuffle & 61.87 ± 1.16 & 57.12 ± 0.90 & 53.49 ± 1.22 & 54.74 ± 2.15 & \\textbf{62.56 ± 0.61} \\\\\n",
      "CIFAR-100 & Random Label & 55.90 ± 1.11 & 56.49 ± 1.83 & 44.03 ± 1.31 & 45.08 ± 1.61 & \\textbf{61.26 ± 1.02} \\\\\n",
      "CIFAR-100 & Batch Label Shuffle & 57.67 ± 0.86 & 56.85 ± 1.13 & 46.37 ± 1.17 & 50.32 ± 1.00 & \\textbf{61.62 ± 1.34} \\\\\n",
      "CIFAR-100 & Batch Label Flip & 48.94 ± 2.13 & 56.34 ± 1.20 & 26.55 ± 7.71 & 24.97 ± 3.29 & \\textbf{59.98 ± 1.06} \\\\\n",
      "CIFAR-100 & Added Noise & 56.83 ± 0.91 & 53.49 ± 1.01 & 48.35 ± 1.48 & 50.58 ± 0.99 & \\textbf{59.56 ± 0.99} \\\\\n",
      "CIFAR-100 & Replace With Noise & \\textbf{60.06 ± 0.97} & 55.00 ± 1.08 & 50.44 ± 2.24 & 53.00 ± 1.31 & 59.99 ± 1.26 \\\\\n",
      "F-MNIST & Original Data & 82.86 ± 1.51 & 80.95 ± 1.40 & \\textbf{83.07 ± 1.06} & 77.85 ± 1.94 & 82.86 ± 1.51 \\\\\n",
      "F-MNIST & Chunk Shuffle & 76.90 ± 2.00 & 76.10 ± 1.15 & 76.82 ± 1.88 & 73.79 ± 3.12 & \\textbf{81.03 ± 2.27} \\\\\n",
      "F-MNIST & Random Label & 73.47 ± 5.78 & 77.58 ± 1.69 & 74.30 ± 5.50 & 67.84 ± 6.00 & \\textbf{77.91 ± 3.39} \\\\\n",
      "F-MNIST & Batch Label Shuffle & 81.26 ± 1.72 & 78.35 ± 1.17 & 81.43 ± 0.54 & 78.79 ± 0.55 & \\textbf{81.53 ± 1.70} \\\\\n",
      "F-MNIST & Batch Label Flip & 77.64 ± 3.43 & 77.84 ± 1.50 & 78.22 ± 3.04 & 75.96 ± 2.83 & \\textbf{79.26 ± 3.78} \\\\\n",
      "F-MNIST & Added Noise & 78.05 ± 1.96 & 73.12 ± 1.96 & 78.10 ± 1.69 & 75.26 ± 1.89 & \\textbf{79.98 ± 3.83} \\\\\n",
      "F-MNIST & Replace With Noise & 78.99 ± 2.20 & 74.29 ± 1.19 & 79.09 ± 1.72 & 75.82 ± 2.14 & \\textbf{81.14 ± 2.25} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_synthetic_results.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7558e63c23b0a0714362031d3a7b8ced60481b05af2dfe8700c0e85c4165f598"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dcarte')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
